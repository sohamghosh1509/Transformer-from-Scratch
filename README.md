# Transformer from Scratch

This project involves implementing the Transformer architecture from scratch, including both the encoder and decoder components. The implementation demonstrates a foundational understanding of sequence-to-sequence modeling.

## Features
- Complete implementation of the Transformer architecture from the ground up.
- Includes both **Encoder** and **Decoder** components.
- Supports positional encodings, multi-head attention, and feed-forward networks as described in the original *"Attention is All You Need"* paper.

## Evaluation
- The model was evaluated on an **English-to-French translation task**.
- Achieved a BLEU score of **0.3**, highlighting the successful implementation and functioning of the architecture.
